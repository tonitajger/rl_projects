{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(obs_vec):\n",
    "    \"\"\"\n",
    "    Encodes an observed state to a number between 0 and 99999 for a given observation.\n",
    "    :param obs_vec: Observation vector given from the environment\n",
    "    :return: integer between 0 and 9999 describing the current state\n",
    "    \"\"\"\n",
    "    disc_vec = (obs_vec / np.array([2.4, 2, 12*math.pi/180, 2]) + 1)/2\n",
    "    disc_vec = np.round(disc_vec, 1)\n",
    "    return int(np.array([1000, 100, 10, 1]).dot(disc_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPole:\n",
    "    \"\"\"\n",
    "    RL classic Q-learning implementation of cart pole problem, balancing a pole upright on a cart on a friction-less\n",
    "    surface. The \"Gym\" package from OpenAI is used as the gym environment for the RL method. This simple Q-learning\n",
    "    method is quite inefficient and under-performing, but illustrates the essential idea of the RL approach.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initiate the CartPole object and the Q-matrix with 0s\n",
    "        \"\"\"\n",
    "        self.Q = np.zeros((10000, 2))\n",
    "\n",
    "    def train(self, n, save_file=None, monitor=False, printout=False):\n",
    "        \"\"\"\n",
    "        Training the agent on the cart pole environment. The environment is reset at time 200, if the angle of the pole\n",
    "        exceeds +-12 degrees or cart position outside +-2.4.\n",
    "        :param n: Number of training episodes\n",
    "        :param save_file: If the Q-table should be saved or not to .pkl file\n",
    "        :param monitor: If the environment should be rendered or not\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        env = gym.make('CartPole-v0')\n",
    "\n",
    "        # Initial hyper parameter values\n",
    "        eps = .5\n",
    "        alpha = 1\n",
    "        gamma = .99\n",
    "\n",
    "        # Save the times to a 1000 dim vector for calculating the average to monitor the training\n",
    "        avg = np.zeros(1000)\n",
    "        for i_episode in range(1, n):\n",
    "\n",
    "            # Alpha and epsilon decreasing with the number of episodes\n",
    "            if alpha > .2:\n",
    "                alpha *= .9995\n",
    "            if eps > .1:\n",
    "                eps *= .9996\n",
    "\n",
    "            # Get the initial state of the cart pole\n",
    "            observation = env.reset()\n",
    "            state = get_state(np.array(observation))\n",
    "\n",
    "            for t in range(1, 1000):\n",
    "\n",
    "                # Monitor every 10th episode if monitor param is True\n",
    "                if monitor and (i_episode % 100 == 0):\n",
    "                    env.render()\n",
    "                    time.sleep(.02)\n",
    "\n",
    "                # Random variable to decide if the agent should explore or exploit\n",
    "                explore = random.random()\n",
    "                if explore < eps:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(self.Q[state])\n",
    "\n",
    "                    # If the maximum is 0 i.e. no value has been set yet to the row, choose action at random\n",
    "                    if not self.Q[state, action]:\n",
    "                        action = random.randint(0, 1)\n",
    "\n",
    "                # Take the decided action and time step\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                new_state = get_state(np.array(observation))\n",
    "\n",
    "                # Tweaking of the reward given: -1 if the pole falls and larger reward when pole angle close to 0\n",
    "                reward -= 5*abs(observation[2])\n",
    "\n",
    "                # Update the Q-value\n",
    "                self.Q[state, action] = self.Q[state, action] + alpha*(reward + gamma*np.amax(self.Q[new_state]) -\n",
    "                                                                       self.Q[state, action])\n",
    "                if done:\n",
    "                    avg[i_episode % 1000] = t\n",
    "                    if printout and (i_episode % 1000 == 0):\n",
    "                        print(\"Iteration: {}\".format(i_episode))\n",
    "                        print(\"Episode finished at average time t = {}\".format(np.mean(avg)))\n",
    "                    break\n",
    "\n",
    "                state = new_state\n",
    "\n",
    "        # Save the Q-table\n",
    "        if save_file:\n",
    "            q_file = open(save_file, \"wb\")\n",
    "            pickle.dump(self.Q, q_file)\n",
    "            q_file.close()\n",
    "\n",
    "        env.close()\n",
    "\n",
    "    def demo(self, n):\n",
    "        \"\"\"\n",
    "        Demonstrates the cart pole performance using the loaded Q-matrix\n",
    "        :param n: Number of episodes to demonstrate\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        env = gym.make('CartPole-v0')\n",
    "\n",
    "        for i in range(n):\n",
    "            observation = env.reset()\n",
    "            state = get_state(np.array(observation))\n",
    "\n",
    "            for t in range(1, 1000):\n",
    "                env.render()\n",
    "                time.sleep(.02)\n",
    "\n",
    "                # Choose action from Q-table\n",
    "                action = np.argmax(self.Q[state])\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                state = get_state(np.array(observation))\n",
    "\n",
    "                if done:\n",
    "                    print(\"Episode finished after {} time steps\".format(t + 1))\n",
    "                    break\n",
    "        env.close()\n",
    "\n",
    "    def load_q_file(self, file_name):\n",
    "        \"\"\"\n",
    "        Load a Q-matrix using a saved .pkl file\n",
    "        :param file_name: Path to the saved Q-matrix file\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        f = open(file_name, \"rb\")\n",
    "        self.Q = pickle.load(f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = CartPole()\n",
    "# cp.train(10000, save_file=\"Q_tables/Q1e4states.pkl\", monitor=False)\n",
    "cp.load_q_file(\"cartpole/Q_tables/Q1e4states.pkl\")\n",
    "cp.demo(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
