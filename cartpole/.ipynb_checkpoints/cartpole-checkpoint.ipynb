{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gym\n",
    "\n",
    "from keras import layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic qlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(obs_vec):\n",
    "    \"\"\"\n",
    "    Encodes an observed state to a number between 0 and 99999 for a given observation.\n",
    "    :param obs_vec: Observation vector given from the environment\n",
    "    :return: integer between 0 and 9999 describing the current state\n",
    "    \"\"\"\n",
    "    disc_vec = (obs_vec / np.array([2.4, 2, 12*math.pi/180, 2]) + 1)/2\n",
    "    disc_vec = np.round(disc_vec, 1)\n",
    "    return int(np.array([1000, 100, 10, 1]).dot(disc_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPole:\n",
    "    \"\"\"\n",
    "    RL classic Q-learning implementation of cart pole problem, balancing a pole upright on a cart on a friction-less\n",
    "    surface. The \"Gym\" package from OpenAI is used as the gym environment for the RL method. This simple Q-learning\n",
    "    method is quite inefficient and under-performing, but illustrates the essential idea of the RL approach.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initiate the CartPole object and the Q-matrix with 0s\n",
    "        \"\"\"\n",
    "        self.Q = np.zeros((10000, 2))\n",
    "\n",
    "    def train(self, n, save_file=None, monitor=False, printout=False):\n",
    "        \"\"\"\n",
    "        Training the agent on the cart pole environment. The environment is reset at time 200, if the angle of the pole\n",
    "        exceeds +-12 degrees or cart position outside +-2.4.\n",
    "        :param n: Number of training episodes\n",
    "        :param save_file: If the Q-table should be saved or not to .pkl file\n",
    "        :param monitor: If the environment should be rendered or not\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        env = gym.make('CartPole-v0')\n",
    "\n",
    "        # Initial hyper parameter values\n",
    "        eps = .5\n",
    "        alpha = 1\n",
    "        gamma = .99\n",
    "\n",
    "        # Save the times to a 1000 dim vector for calculating the average to monitor the training\n",
    "        avg = np.zeros(1000)\n",
    "        for i_episode in range(1, n):\n",
    "\n",
    "            # Alpha and epsilon decreasing with the number of episodes\n",
    "            if alpha > .2:\n",
    "                alpha *= .9995\n",
    "            if eps > .1:\n",
    "                eps *= .9996\n",
    "\n",
    "            # Get the initial state of the cart pole\n",
    "            observation = env.reset()\n",
    "            state = get_state(np.array(observation))\n",
    "\n",
    "            for t in range(1, 1000):\n",
    "\n",
    "                # Monitor every 10th episode if monitor param is True\n",
    "                if monitor and (i_episode % 100 == 0):\n",
    "                    env.render()\n",
    "                    time.sleep(.02)\n",
    "\n",
    "                # Random variable to decide if the agent should explore or exploit\n",
    "                explore = random.random()\n",
    "                if explore < eps:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(self.Q[state])\n",
    "\n",
    "                    # If the maximum is 0 i.e. no value has been set yet to the row, choose action at random\n",
    "                    if not self.Q[state, action]:\n",
    "                        action = random.randint(0, 1)\n",
    "\n",
    "                # Take the decided action and time step\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                new_state = get_state(np.array(observation))\n",
    "\n",
    "                # Tweaking of the reward given: -1 if the pole falls and larger reward when pole angle close to 0\n",
    "                reward -= 5*abs(observation[2])\n",
    "\n",
    "                # Update the Q-value\n",
    "                self.Q[state, action] = self.Q[state, action] + alpha*(reward + gamma*np.amax(self.Q[new_state]) -\n",
    "                                                                       self.Q[state, action])\n",
    "                if done:\n",
    "                    avg[i_episode % 1000] = t\n",
    "                    if printout and (i_episode % 1000 == 0):\n",
    "                        print(\"Iteration: {}\".format(i_episode))\n",
    "                        print(\"Episode finished at average time t = {}\".format(np.mean(avg)))\n",
    "                    break\n",
    "\n",
    "                state = new_state\n",
    "\n",
    "        # Save the Q-table\n",
    "        if save_file:\n",
    "            q_file = open(save_file, \"wb\")\n",
    "            pickle.dump(self.Q, q_file)\n",
    "            q_file.close()\n",
    "\n",
    "        env.close()\n",
    "\n",
    "    def demo(self, n):\n",
    "        \"\"\"\n",
    "        Demonstrates the cart pole performance using the loaded Q-matrix\n",
    "        :param n: Number of episodes to demonstrate\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        env = gym.make('CartPole-v0')\n",
    "\n",
    "        for i in range(n):\n",
    "            observation = env.reset()\n",
    "            state = get_state(np.array(observation))\n",
    "\n",
    "            for t in range(1, 1000):\n",
    "                env.render()\n",
    "                time.sleep(.02)\n",
    "\n",
    "                # Choose action from Q-table\n",
    "                action = np.argmax(self.Q[state])\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                state = get_state(np.array(observation))\n",
    "\n",
    "                if done:\n",
    "                    print(\"Episode finished after {} time steps\".format(t + 1))\n",
    "                    break\n",
    "        env.close()\n",
    "\n",
    "    def load_q_file(self, file_name):\n",
    "        \"\"\"\n",
    "        Load a Q-matrix using a saved .pkl file\n",
    "        :param file_name: Path to the saved Q-matrix file\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        f = open(file_name, \"rb\")\n",
    "        self.Q = pickle.load(f)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = CartPole()\n",
    "# cp.train(10000, save_file=\"Q_tables/Q1e4states.pkl\", monitor=False)\n",
    "cp.load_q_file(\"cartpole/Q_tables/Q1e4states.pkl\")\n",
    "cp.demo(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep qlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCartPole:\n",
    "    \n",
    "\n",
    "    def __init__(self):\n",
    "        self.Q = None\n",
    "        self.Q_target = None\n",
    "\n",
    "    def train(self, env, eps=.5, alpha=1, gamma=.99, monitor=False):\n",
    "        MINI_SIZE = 64\n",
    "\n",
    "        model = models.Sequential()\n",
    "        model.add(layers.Dense(10, activation='relu',\n",
    "                               input_shape=env.observation_space.shape))\n",
    "        model.add(layers.Dense(10, activation='relu'))\n",
    "        model.add(layers.Dense(env.action_space.n, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer='rmsprop')\n",
    "\n",
    "        Q = model\n",
    "        Q_target = model\n",
    "\n",
    "        samples = deque(maxlen=1000)\n",
    "\n",
    "        for i_episode in range(1, 100):\n",
    "\n",
    "            # Get the initial state of the cart pole\n",
    "            observation = env.reset()\n",
    "            state = np.array(observation)\n",
    "\n",
    "            for t in range(1, 1000):\n",
    "\n",
    "                # Monitor every 10th episode if monitor param is True\n",
    "                if monitor and (i_episode % 10 == 0):\n",
    "                    env.render()\n",
    "                    time.sleep(.02)\n",
    "\n",
    "                # Random variable to decide if the agent should explore or exploit\n",
    "                explore = random.random()\n",
    "                if explore < eps:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(Q.predict(np.array([observation])))\n",
    "\n",
    "                # Take the decided action and time step\n",
    "                observation, reward, done, info = env.step(action)\n",
    "                new_state = np.array(observation)\n",
    "                sars = SARS(state, action, reward, new_state)\n",
    "                samples.append(sars)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "                    #avg[i_episode % 1000] = t\n",
    "#                     if printout and (i_episode % 1000 == 0):\n",
    "#                         print(\"Iteration: {}\".format(i_episode))\n",
    "#                         print(\"Episode finished at average time t = {}\".format(\n",
    "#                             np.mean(avg)))\n",
    "#                     break\n",
    "                state = new_state\n",
    "            \n",
    "            if len(samples) > MINI_SIZE:\n",
    "                mini_batch = random.sample(samples, MINI_SIZE)\n",
    "                states = []\n",
    "                rewards = []\n",
    "                actions = []\n",
    "                new_states = []\n",
    "\n",
    "                for sars in mini_batch:\n",
    "                    states.append(sars.s)\n",
    "                    rewards.append(sars.a)\n",
    "                    actions.append(sars.r)\n",
    "                    new_states.append(sars.sn)\n",
    "                \n",
    "                states = np.array(states)\n",
    "                rewards = np.array(rewards)\n",
    "                actions = np.array(actions)\n",
    "                new_states = np.array(new_states)\n",
    "                \n",
    "                Q_pred = Q.predict(states)\n",
    "                labels = Q_pred\n",
    "                label_idxs = np.argmax(Q_target.predict(new_states), axis=1)\n",
    "                label_values = rewards + gamma * np.amax(Q_target.predict(new_states), axis=1)\n",
    "                \n",
    "                for i, (idx, value) in enumerate(zip(label_idxs, label_values)):\n",
    "                    labels[i][idx] = value\n",
    "                \n",
    "                Q.fit(states, labels, epochs=1000, verbose=0, batch_size = 64)\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "class SARS:\n",
    "    def __init__(self, s, a, r, sn):\n",
    "        self.s = s\n",
    "        self.a = a\n",
    "        self.r = r\n",
    "        self.sn = sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-f5aa612f9e37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CartPole-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdeep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepCartPole\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdeep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-164-6d22b23894e6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, env, eps, alpha, gamma, monitor)\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/Applications/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;34m\"\"\"Resets the state of metrics.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_training_eval_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_get_training_eval_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    723\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_output_loss_metrics'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_loss_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'metrics'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m             \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Applications/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mmetrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;34m\"\"\"Returns the model's metrics added using `compile`, `add_metric` APIs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "deep = DeepCartPole()\n",
    "deep.train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
